{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panama papers: the names (web scraping)\n",
    "\n",
    "([Data Source: Sunday Times List of Names](http://features.thesundaytimes.co.uk/web/public/2016/04/10/index.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "from itertools import izip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = ZipFile('../data/sunday_times_panama_data.zip')\n",
    "df = pd.read_csv(z.open('sunday_times_panama_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see if all company url's are from opencorporates.com\n",
    "opencorp = df['company_url'].str.find('opencorporates.com') > 0\n",
    "print opencorp.sum()\n",
    "print opencorp.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape company info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client['panama']\n",
    "coll = db['company_reps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_opencorp(istart, iend):\n",
    "    for i in xrange(istart, iend):\n",
    "        request = requests.get(df.ix[i, 'company_url'])\n",
    "        \n",
    "        if request.status_code != 200: # not 'OK'\n",
    "            print 'Error (Row %i): status code', request.status_code\n",
    "        \n",
    "        soup = BeautifulSoup(request.content, 'html.parser')        \n",
    "        tags = soup.select('#attributes')\n",
    "        \n",
    "        if len(tags) > 1:\n",
    "            print 'Error (row %i): more than one attributes!' % i\n",
    "            \n",
    "        elif len(tags) < 1:\n",
    "            print 'Error (row %i): no tag found' % i\n",
    "            continue\n",
    "        \n",
    "        ## save all company info first\n",
    "        coll.insert_one( { 'row_id'      : i,\n",
    "                           'company_name': df.ix[i, 'company_name'],\n",
    "                           'company_info': str(tags[0])              } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# istart = 0\n",
    "istart = 2426\n",
    "edges = np.linspace(istart, len(df), len(df)/1000 + 1, dtype=int, endpoint=True)\n",
    "\n",
    "for bin_edges in izip(edges[:-1], edges[1:]):\n",
    "    print 'Scraping rows [%i, %i)' % bin_edges\n",
    "    scrape_opencorp(*bin_edges)\n",
    "    \n",
    "    sleep(1) # in case pinning too frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***IP was blocked after 2426 requests. Restore database to 2426 entries:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# double check entries to remove\n",
    "\n",
    "print 'expected no. of rows to remove:', coll.count() - istart\n",
    "print 'actual no. of rows to remove:', coll.count( { 'row_id': { '$gt': 2425 } } )\n",
    "print 'rows:', [entry['row_id'] for entry in coll.find( { 'row_id': { '$gt': 2425 } }, { 'row_id' : 1 } )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coll.delete_many( { 'row_id': { '$gt': 2425 } } )\n",
    "print 'new row count:', coll.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HOW TO PREVENT GETTING BLACKLISTED WHILE SCRAPING](https://learn.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/):\n",
    "\n",
    "* OpenCorporates' [robots.txt](https://opencorporates.com/robots.txt)\n",
    "    ```\n",
    "    User-Agent: *\n",
    "    Disallow: /*?page=\n",
    "    Disallow: /*&page=\n",
    "    Disallow: /*/network.json\n",
    "    Sitemap: http://opencorporates.com/sitemap_index.xml.gz\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape with Anonymous Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../auth/tor/tor.json', 'r') as f:\n",
    "    PASSPHRASE = json.load(f)['passphrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "import socks # you need to install pysocks\n",
    "\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "controller = Controller.from_port(port=9051)\n",
    "\n",
    "# you can't open a new controller once you've connected to tor;\n",
    "# try opening a controller right at the top of your script,\n",
    "# then both the tor connection and signaller use the same Controller object\n",
    "controller = Controller.from_port(port=9051)\n",
    "\n",
    "def connectTor():\n",
    "    # Tor Configuration\n",
    "    SOCKS5_PROXY_HOST = '127.0.0.1'\n",
    "    SOCKS5_PROXY_PORT = 9050\n",
    "\n",
    "    # Set up a proxy\n",
    "    socks.set_default_proxy(socks.SOCKS5, SOCKS5_PROXY_HOST, SOCKS5_PROXY_PORT)\n",
    "    socket.socket = socks.socksocket\n",
    "\n",
    "def renew_tor():\n",
    "    controller.authenticate(PASSPHRASE)\n",
    "    controller.signal(Signal.NEWNYM)\n",
    "\n",
    "def showmyip():\n",
    "    print requests.get('http://icanhazip.com/', timeout=24).text # outputs proxy IP\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# istart = 0\n",
    "istart = 2426\n",
    "edges = [2426, 2427, 2428] # test if anonymous crawler works\n",
    "\n",
    "for bin_edges in izip(edges[:-1], edges[1:]):\n",
    "\n",
    "    # renew IP\n",
    "    renew_tor()\n",
    "    connectTor()\n",
    "    print 'Current IP', showmyip()\n",
    "\n",
    "    print 'Scraping rows [%i, %i)' % bin_edges\n",
    "    scrape_opencorp(*bin_edges)\n",
    "    \n",
    "    sleep(10) # avoid renewing IP too frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***OpenCorporates many IPs...***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client['panama']\n",
    "coll = db['company_reps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(coll.count()):\n",
    "    \n",
    "    # fetch scraped data\n",
    "    data = coll.find_one( {'row_id': i} )\n",
    "    soup = BeautifulSoup(data['company_info'], 'html.parser')\n",
    "    tags = soup.select('.officers .attribute_item')\n",
    "    \n",
    "    if not tags:\n",
    "        print 'Error (row %i): no officers found.' % i\n",
    "        continue\n",
    "        \n",
    "    # parse officers position and name\n",
    "    officers = dict()\n",
    "    for tag in tags:\n",
    "        t = [desc.string for desc in tag.children]\n",
    "        officers[t[1][2:]] = t[0]\n",
    "        \n",
    "    # update entry\n",
    "    coll.update_one( { 'row_id': i }, {'$set' : { \"officers\": officers } })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check if all have been parsed \n",
    "print coll.count( { \"officers\" : { '$exists' : True, '$ne' : None } } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare network edge files (.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs  # need to write unicode because some names have accents, ligatures, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = coll.find({}, { 'company_name': True, 'officers': True } )\n",
    "\n",
    "# use tab as delimiter since there might be commas in company name\n",
    "officer_count = 0\n",
    "\n",
    "# two layouts of graph data\n",
    "outfile1 = codecs.open(\"../data/officers_edges.tsv\", \"w\", encoding='utf-8')\n",
    "outfile2 = codecs.open(\"../data/officers_bipartite.tsv\", \"w\", encoding='utf-8')\n",
    "\n",
    "for result in results:\n",
    "    for position, name in result['officers'].iteritems():\n",
    "        outfile1.write(\"%s\\t%s\\n\" % (name, result['company_name']))\n",
    "        outfile2.write(\"%s\\t%s\\t%s\\n\" % (name, result['company_name'], position))\n",
    "        officer_count += 1\n",
    "\n",
    "outfile1.close()\n",
    "outfile2.close()\n",
    "\n",
    "print 'No. of officers:', officer_count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
